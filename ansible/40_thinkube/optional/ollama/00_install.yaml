# Copyright 2025 Alejandro Martínez Corriá and the Thinkube contributors
# SPDX-License-Identifier: Apache-2.0

---
# ansible/40_thinkube/optional/ollama/00_install.yaml
# Description:
#   Orchestrator playbook that runs all Ollama component playbooks in sequence
#   This installs Ollama fast LLM inference server with GPU acceleration
#
# Requirements:
#   - Kubernetes (k8s-snap) must be installed and running
#   - Ingress controller must be deployed
#   - Harbor must be available (for mirrored image)
#   - GPU node available
#
# Usage:
#   cd ~/thinkube
#   ./scripts/run_ansible.sh ansible/40_thinkube/optional/ollama/00_install.yaml
#
# Variables from inventory:
#   - All variables required by individual playbooks
#

- name: Ollama - Deploy
  import_playbook: 10_deploy.yaml

- name: Ollama - Configure Service Discovery
  import_playbook: 17_configure_discovery.yaml
