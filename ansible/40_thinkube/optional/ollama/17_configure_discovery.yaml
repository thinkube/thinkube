# Copyright 2025 Alejandro Martínez Corriá and the Thinkube contributors
# SPDX-License-Identifier: Apache-2.0

---
# ansible/40_thinkube/optional/ollama/17_configure_discovery.yaml
# Description:
#   Configure service discovery for Ollama by creating a ConfigMap
#   that describes the service endpoints and metadata.
#   This enables environment variables (OLLAMA_URL, OLLAMA_API_URL)
#   to be injected into JupyterHub pods.
#
# Requirements:
#   - Ollama must be deployed
#   - Kubernetes (k8s-snap) kubectl access
#
# Usage:
#   cd ~/thinkube
#   ./scripts/run_ansible.sh ansible/40_thinkube/optional/ollama/17_configure_discovery.yaml
#
# Variables from inventory:
#   - domain_name: Base domain for the cluster
#   - kubeconfig: Path to kubeconfig file

- name: Configure Ollama service discovery
  hosts: k8s_control_plane
  gather_facts: true

  vars:
    component_version: "{{ lookup('file', playbook_dir + '/VERSION') }}"
    k8s_namespace: "{{ ollama_namespace | default('ollama') }}"
    ollama_host: "{{ ollama_hostname | default('ollama.' + domain_name) }}"

  tasks:
    - name: Create service discovery ConfigMap
      kubernetes.core.k8s:
        kubeconfig: "{{ kubeconfig }}"
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: thinkube-service-config
            namespace: "{{ k8s_namespace }}"
            labels:
              thinkube.io/managed: "true"
              thinkube.io/service-type: "optional"
              thinkube.io/service-name: "ollama"
              thinkube.io/component: "ollama"
              thinkube.io/component-version: "{{ component_version }}"
          data:
            service.yaml: |
              service:
                name: ollama
                display_name: "Ollama"
                description: "Fast local LLM inference server with GPU acceleration"
                type: optional
                category: ai
                icon: /icons/tk_ai.svg
                component_version: "{{ component_version }}"

                endpoints:
                  - name: api
                    type: http
                    url: "http://ollama.{{ k8s_namespace }}.svc.cluster.local:11434"
                    health_url: "http://ollama.{{ k8s_namespace }}.svc.cluster.local:11434/"
                    description: "Ollama API (OpenAI-compatible at /v1)"
                    primary: true

                  - name: external
                    type: http
                    url: "https://{{ ollama_host }}"
                    health_url: "https://{{ ollama_host }}/"
                    description: "External API endpoint"
                    primary: false

                dependencies: []

                scaling:
                  resource_type: statefulset
                  resource_name: ollama
                  namespace: "{{ k8s_namespace }}"
                  min_replicas: 1
                  can_disable: true

                environment_variables:
                  - name: OLLAMA_URL
                    value: "http://ollama.{{ k8s_namespace }}.svc.cluster.local:11434"
                  - name: OLLAMA_API_URL
                    value: "http://ollama.{{ k8s_namespace }}.svc.cluster.local:11434/v1"

    - name: Verify ConfigMap creation
      kubernetes.core.k8s_info:
        kubeconfig: "{{ kubeconfig }}"
        api_version: v1
        kind: ConfigMap
        name: thinkube-service-config
        namespace: "{{ k8s_namespace }}"
      register: configmap_info

    - name: Display ConfigMap status
      ansible.builtin.debug:
        msg: "Service discovery ConfigMap created for Ollama"
      when: configmap_info.resources | length > 0

    - name: Update code-server environment variables
      include_role:
        name: code_server_env_update
