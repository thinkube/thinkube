# Copyright 2025 Alejandro Martínez Corriá and the Thinkube contributors
# SPDX-License-Identifier: Apache-2.0

# TensorRT-LLM base image with NVFP4 support for Blackwell GPUs
# Based on NVIDIA NGC TensorRT-LLM 1.2.0rc2 mirrored to Harbor
FROM {{ harbor_registry }}/library/tensorrt-llm:1.2.0rc2

# Set working directory
WORKDIR /app

# Pre-install common Python dependencies for LLM serving templates
# These are commonly used across all TensorRT-LLM deployments
RUN pip install --no-cache-dir \
    openai>=1.0.0 \
    gradio>=4.42.0

# Create icons directory for Thinkube branding
RUN mkdir -p /app/icons

# The base image already includes:
# - TensorRT-LLM runtime and tools
# - CUDA 13.0 with Blackwell (sm_121a) support
# - Python 3.12
# - All TensorRT-LLM dependencies

# This image is used as base for tkt-tensorrt-llm template deployments
# Templates only need to:
# 1. Copy their application code (server.py, entrypoint.sh, etc.)
# 2. Copy icons
# 3. Install any additional template-specific requirements
# 4. Set MODEL_ID via environment variable (not baked into image)
