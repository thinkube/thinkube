# Copyright 2025 Alejandro Martínez Corriá and the Thinkube contributors
# SPDX-License-Identifier: Apache-2.0

# AI/ML inference base image with tk-vllm for DGX Spark GB10
# Architecture: ARM64 only (x86-64 support coming in future version)
# Based on community patches from: github.com/eelbaz/dgx-spark-vllm-setup

FROM {{ harbor_registry }}/library/cuda:13.0.0-devel-ubuntu24.04

# Build arguments
ARG TARGETARCH
ARG TK_VLLM_VERSION=0.11.1rc5
ARG PYTHON_VERSION=3.12

# Prevent interactive prompts during build
ENV DEBIAN_FRONTEND=noninteractive

# Architecture validation - ARM64 only for now
RUN if [ "$TARGETARCH" != "arm64" ]; then \
        echo "ERROR: This image currently only supports ARM64 (DGX Spark GB10)"; \
        echo "x86-64 support (RTX 20/30/40/50 series) will be added in future version"; \
        exit 1; \
    fi

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    python${PYTHON_VERSION} \
    python${PYTHON_VERSION}-dev \
    python${PYTHON_VERSION}-venv \
    python3-pip \
    git \
    wget \
    curl \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.12 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 && \
    update-alternatives --install /usr/bin/python python /usr/bin/python${PYTHON_VERSION} 1

# Upgrade pip (ignore system packages to avoid Debian package conflicts)
RUN python3 -m pip install --no-cache-dir --break-system-packages --ignore-installed --upgrade pip setuptools wheel

# Set CUDA environment variables
ENV CUDA_HOME=/usr/local/cuda-13.0
ENV PATH="${CUDA_HOME}/bin:${PATH}"
ENV LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}"
ENV TRITON_PTXAS_PATH="${CUDA_HOME}/bin/ptxas"

# Enable vLLM V1 engine and FlashInfer MXFP4 MOE
ENV VLLM_USE_V1=1
ENV VLLM_USE_FLASHINFER_MXFP4_MOE=1
ENV VLLM_TARGET_DEVICE=cuda

# Install PyTorch with CUDA 13.0 support
RUN pip3 install --no-cache-dir --break-system-packages \
    torch==2.9.0 \
    torchvision==0.24.0 \
    torchaudio==2.9.0 \
    --index-url https://download.pytorch.org/whl/cu130

# Install vllm from pre-built wheel
# Wheel includes patches for DGX Spark GB10 (Blackwell sm_121a support)
RUN pip3 install --no-cache-dir --break-system-packages \
    https://github.com/thinkube/tk-vllm-wheels/releases/download/v${TK_VLLM_VERSION}/vllm-0.11.1rc6.dev0+g2918c1b49.d20251112.cu130-cp312-cp312-linux_aarch64.whl

# Install additional ML packages
RUN pip3 install --no-cache-dir --break-system-packages \
    transformers>=4.56.1 \
    accelerate>=0.33.0 \
    xgrammar>=0.1.24 \
    gradio>=4.42.0 \
    fastapi>=0.112.0 \
    uvicorn[standard]>=0.30.5

# Install additional utilities
RUN pip3 install --no-cache-dir --break-system-packages \
    numpy \
    pillow \
    requests \
    pyyaml \
    huggingface-hub \
    safetensors \
    sentencepiece \
    protobuf \
    psutil

# Verify installation
RUN echo "=== Verifying tk-vLLM Installation ===" && \
    python3 -c "import vllm; print(f'✓ vLLM version: {vllm.__version__}')" && \
    python3 -c "import torch; print(f'✓ PyTorch version: {torch.__version__}')" && \
    python3 -c "import torch; print(f'✓ CUDA available: {torch.cuda.is_available()}')" && \
    python3 -c "import torch; print(f'✓ CUDA version: {torch.version.cuda}')" && \
    echo "=== Installation Verified ==="

# Set application environment variables
ENV PYTHONPATH=/app
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV GRADIO_SERVER_NAME=0.0.0.0
ENV GRADIO_SERVER_PORT=7860
ENV HF_HOME=/app/cache
ENV TRANSFORMERS_CACHE=/app/cache

# Create application directories
RUN mkdir -p /app/cache /app/models

WORKDIR /app

# Expose default Gradio port
EXPOSE 7860

# Default healthcheck (can be overridden)
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:7860/health || exit 1

# Default command
CMD ["/bin/bash"]

# ==============================================================================
# NOTE: x86-64 Support (RTX 20/30/40/50 Series)
# ==============================================================================
# Future version will add x86-64 support with pre-built tk-vllm wheel
# Requires: tk_vllm-{version}-cp312-cp312-linux_x86_64.whl
# GPU architectures: 7.5 (RTX 20), 8.6 (RTX 30), 8.9 (RTX 40), 9.0 (RTX 50)
# ==============================================================================
