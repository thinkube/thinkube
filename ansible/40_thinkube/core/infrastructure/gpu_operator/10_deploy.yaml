# Copyright 2025 Alejandro MartÃ­nez CorriÃ¡ and the Thinkube contributors
# SPDX-License-Identifier: Apache-2.0

---
# ansible/40_thinkube/core/infrastructure/gpu_operator/10_deploy.yaml
# Description:
#   Installs and configures NVIDIA GPU Operator on Kubernetes cluster
#
# Requirements:
#   - Canonical Kubernetes (k8s-snap) >= 1.34
#   - NVIDIA drivers >= 470.x already installed on the host system
#   - kubectl CLI (provided by k8s-snap)
#   - Helm >= 3.7.0
#   - jq (for JSON parsing)
#
# Usage:
#   cd ~/thinkube
#   ./scripts/run_ansible.sh ansible/40_thinkube/core/infrastructure/gpu_operator/10_deploy.yaml
#
# Variables from inventory:
#   - kubeconfig: Path to Kubernetes configuration file
#   - kubectl_bin: Path to kubectl binary
#   - helm_bin: Path to Helm binary
#   - gpu_operator_version: (Optional) Version of GPU Operator to install
#
# ðŸ¤– [AI-assisted]

- name: Install NVIDIA GPU Operator
  hosts: k8s_control_plane
  become: false

  pre_tasks:
    - name: Verify required variables exist
      ansible.builtin.fail:
        msg: "Required variable {{ item }} is not defined"
      when: item is not defined
      loop:
        - kubeconfig
        - kubectl_bin
        - helm_bin

  vars:
    user: "{{ system_username }}"
    gpu_operator_namespace: gpu-operator
    cuda_test_namespace: default
    cuda_test_manifest: /tmp/cuda-vectoradd.yaml
    cuda_test_pod: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: cuda-vectoradd
        namespace: {{ cuda_test_namespace }}
      spec:
        restartPolicy: OnFailure
        runtimeClassName: nvidia
        containers:
        - name: cuda-vectoradd
          image: "nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda12.5.0-ubuntu22.04"
          resources:
            limits:
              nvidia.com/gpu: 1
            requests:
              nvidia.com/gpu: 1

  tasks:
    - name: Check if GPU Operator is already installed
      ansible.builtin.command: "{{ kubectl_bin }} get namespace {{ gpu_operator_namespace }}"
      register: gpu_ns_check
      failed_when: false
      changed_when: false

    - name: Add NVIDIA Helm repository
      ansible.builtin.command: "{{ helm_bin }} repo add nvidia https://nvidia.github.io/gpu-operator --force-update"
      changed_when: false

    - name: Update Helm repositories
      ansible.builtin.command: "{{ helm_bin }} repo update"
      changed_when: false

    - name: Debug - Show kubectl/helm/kubeconfig values
      ansible.builtin.debug:
        msg:
          - "kubectl_bin: {{ kubectl_bin }}"
          - "helm_bin: {{ helm_bin }}"
          - "kubeconfig: {{ kubeconfig }}"
          - "system_username: {{ system_username }}"

    # ===========================
    # Configure Docker for GPU Access (DGX Spark NIM Support)
    # ===========================
    - name: Configure Docker daemon for NVIDIA runtime
      ansible.builtin.copy:
        content: |
          {
              "runtimes": {
                  "nvidia": {
                      "path": "nvidia-container-runtime",
                      "runtimeArgs": []
                  }
              }
          }
        dest: /etc/docker/daemon.json
        mode: '0644'
      become: true

    - name: Restart Docker to load NVIDIA runtime
      ansible.builtin.systemd:
        name: docker
        state: restarted
      become: true

    # ===========================
    # Install GPU Operator
    # ===========================
    - name: Install GPU Operator
      ansible.builtin.shell: |
        {{ helm_bin }} upgrade --install gpu-operator nvidia/gpu-operator \
          --namespace {{ gpu_operator_namespace }} \
          --create-namespace \
          --wait \
          --timeout 15m \
          --set driver.enabled=false \
          --set toolkit.env[0].name=CONTAINERD_CONFIG \
          --set toolkit.env[0].value=/var/lib/k8s-containerd/k8s-containerd/etc/containerd/config.toml \
          --set toolkit.env[1].name=CONTAINERD_SOCKET \
          --set toolkit.env[1].value=/var/lib/k8s-containerd/k8s-containerd/run/containerd/containerd.sock \
          --set toolkit.env[2].name=CONTAINERD_SET_AS_DEFAULT \
          --set-string toolkit.env[2].value=true \
          --set toolkit.env[3].name=RUNTIME_DROP_IN_CONFIG_HOST_PATH \
          --set toolkit.env[3].value=/var/lib/k8s-containerd/k8s-containerd/etc/containerd/conf.d/99-nvidia.toml \
          --set-json 'daemonsets.toolkit.volumes=[{"name":"containerd-config","hostPath":{"path":"/var/lib/k8s-containerd/k8s-containerd/etc/containerd","type":"DirectoryOrCreate"}},{"name":"containerd-drop-in-config","hostPath":{"path":"/var/lib/k8s-containerd/k8s-containerd/etc/containerd/conf.d","type":"DirectoryOrCreate"}},{"name":"containerd-socket","hostPath":{"path":"/var/lib/k8s-containerd/k8s-containerd/run/containerd"}}]'
      environment:
        KUBECONFIG: "{{ kubeconfig }}"
      register: gpu_operator_install
      changed_when: "'STATUS: deployed' in gpu_operator_install.stdout or 'has been upgraded' in gpu_operator_install.stdout"

    - name: Wait for GPU operator namespace to be ready
      ansible.builtin.pause:
        seconds: 30
        prompt: "Waiting for GPU operator to initialize pods..."

    - name: Wait for nvidia-container-toolkit daemonset to be running
      ansible.builtin.shell: |
        # Wait for the toolkit daemonset to be running
        i=1
        while [ $i -le 60 ]; do
          count=$({{ kubectl_bin }} get pods -n {{ gpu_operator_namespace }} -l app=nvidia-container-toolkit-daemonset --field-selector=status.phase=Running -o json | jq '.items | length')
          if [ "$count" -ge 1 ]; then
            echo "âœ“ nvidia-container-toolkit daemonset is running"
            exit 0
          fi
          echo "Waiting for nvidia-container-toolkit daemonset... (attempt $i/60)"
          sleep 5
          i=$((i + 1))
        done
        echo "ERROR: nvidia-container-toolkit-daemonset not running"
        exit 1
      register: toolkit_wait
      changed_when: false

    - name: Wait for nvidia-container-toolkit to create drop-in config
      ansible.builtin.wait_for:
        path: /etc/containerd/conf.d/99-nvidia.toml
        state: present
        timeout: 300
      become: true

    - name: Restart k8s-snap containerd to pick up nvidia runtime configuration
      ansible.builtin.command: snap restart k8s.containerd
      become: true

    - name: Wait for k8s-snap containerd to be ready after restart
      ansible.builtin.pause:
        seconds: 10
        prompt: "Waiting for k8s-snap containerd to restart..."

    - name: Check GPU operator deployment status
      ansible.builtin.shell: |
        {{ kubectl_bin }} get pods -n {{ gpu_operator_namespace }} -o wide
        echo "---"
        {{ kubectl_bin }} get nodes -o custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu
      register: gpu_status
      changed_when: false

    - name: Display GPU operator status
      ansible.builtin.debug:
        var: gpu_status.stdout_lines

    - name: Wait for critical GPU operator pods
      ansible.builtin.shell: |
        # Wait for nvidia-device-plugin-daemonset pods
        device_ready=false
        i=1
        while [ $i -le 30 ]; do
          count=$({{ kubectl_bin }} get pods -n {{ gpu_operator_namespace }} -l app=nvidia-device-plugin-daemonset --field-selector=status.phase=Running -o json | jq '.items | length')
          if [ "$count" -ge 1 ]; then
            device_ready=true
            break
          fi
          echo "Waiting for nvidia-device-plugin-daemonset pods... (attempt $i/30)"
          sleep 5
          i=$((i + 1))
        done
        
        # Wait for nvidia-container-toolkit-daemonset pods
        toolkit_ready=false
        i=1
        while [ $i -le 30 ]; do
          count=$({{ kubectl_bin }} get pods -n {{ gpu_operator_namespace }} -l app=nvidia-container-toolkit-daemonset --field-selector=status.phase=Running -o json | jq '.items | length')
          if [ "$count" -ge 1 ]; then
            toolkit_ready=true
            break
          fi
          echo "Waiting for nvidia-container-toolkit-daemonset pods... (attempt $i/30)"
          sleep 5
          i=$((i + 1))
        done
        
        if [ "$device_ready" = "true" ] && [ "$toolkit_ready" = "true" ]; then
          echo "âœ“ All critical GPU operator pods are running"
          exit 0
        else
          echo "ERROR: GPU operator pods did not become ready in time"
          exit 1
        fi
      register: wait_result
      changed_when: false

    - name: Skip CUDA validator wait if GPUs already available
      ansible.builtin.shell: |
        # Check if GPUs are already available on nodes
        gpu_count=$({{ kubectl_bin }} get nodes -o json | jq '[.items[].status.allocatable."nvidia.com/gpu" // "0" | tonumber] | add')
        if [ "$gpu_count" -gt 0 ]; then
          echo "âœ“ GPUs already available on nodes (count: $gpu_count)"
          echo "skip_validator=true"
        else
          echo "No GPUs detected yet, will wait for validator"
          echo "skip_validator=false"
        fi
      register: gpu_check
      changed_when: false

    - name: Wait for CUDA validator completion
      ansible.builtin.shell: |
        i=1
        while [ $i -le 30 ]; do
          # Check for any validator pods (Running or Succeeded)
          count=$({{ kubectl_bin }} get pods -n {{ gpu_operator_namespace }} -l app=nvidia-cuda-validator -o json | jq '.items | length')
          succeeded=$({{ kubectl_bin }} get pods -n {{ gpu_operator_namespace }} -l app=nvidia-cuda-validator --field-selector=status.phase=Succeeded -o json | jq '.items | length')
          
          if [ "$succeeded" -ge 1 ] || [ "$count" -eq 0 ]; then
            echo "âœ“ CUDA validator completed or not needed"
            exit 0
          fi
          
          echo "Waiting for CUDA validator... (attempt $i/30, found $count validator pods)"
          sleep 5
          i=$((i + 1))
        done
        
        echo "WARNING: CUDA validator did not complete in time, but continuing..."
        exit 0
      register: validator_result
      changed_when: false
      when: "'skip_validator=false' in gpu_check.stdout"

    - name: Wait for GPUs to become available in cluster
      ansible.builtin.shell: |
        echo "Waiting for GPUs to become available in the cluster..."
        i=1
        max_attempts=60
        while [ $i -le $max_attempts ]; do
          # Check if GPUs are available on any node
          gpu_count=$({{ kubectl_bin }} get nodes -o json | jq '[.items[].status.allocatable."nvidia.com/gpu" // "0" | tonumber] | add')
          
          if [ "$gpu_count" -gt 0 ]; then
            echo "âœ“ GPUs are now available in the cluster (count: $gpu_count)"
            # Show which nodes have GPUs
            echo "GPU-enabled nodes:"
            {{ kubectl_bin }} get nodes -o custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu | grep -v "<none>" || true
            exit 0
          fi
          
          # Check GPU operator pod status
          running_pods=$({{ kubectl_bin }} get pods -n {{ gpu_operator_namespace }} --field-selector=status.phase=Running -o json | jq '.items | length')
          total_pods=$({{ kubectl_bin }} get pods -n {{ gpu_operator_namespace }} -o json | jq '.items | length')
          
          echo "Waiting for GPUs... (attempt $i/$max_attempts, $running_pods/$total_pods pods running)"
          
          # Show current operator pod status every 10 attempts
          if [ $((i % 10)) -eq 0 ]; then
            echo "Current GPU operator pod status:"
            {{ kubectl_bin }} get pods -n {{ gpu_operator_namespace }} --no-headers | awk '{print "  " $1 ": " $3}'
          fi
          
          sleep 10
          i=$((i + 1))
        done
        
        echo "ERROR: No GPUs detected after waiting 10 minutes."
        echo "Current node status:"
        {{ kubectl_bin }} get nodes -o custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu
        echo ""
        echo "GPU operator pod status:"
        {{ kubectl_bin }} get pods -n {{ gpu_operator_namespace }} --no-headers
        exit 1
      register: gpu_wait_result
      changed_when: false
      failed_when: gpu_wait_result.rc != 0

    - name: Check if we should run CUDA test
      ansible.builtin.shell: |
        # Check if GPUs are available
        gpu_count=$({{ kubectl_bin }} get nodes -o json | jq '[.items[].status.allocatable."nvidia.com/gpu" // "0" | tonumber] | add')
        
        # Check if test pod already exists
        existing_pod=$({{ kubectl_bin }} get pod cuda-vectoradd -n {{ cuda_test_namespace }} 2>/dev/null || echo "not found")
        
        if [ "$gpu_count" -eq 0 ]; then
          echo "skip_test=true"
          echo "reason=No GPUs available"
        elif echo "$existing_pod" | grep -q "cuda-vectoradd"; then
          echo "skip_test=true" 
          echo "reason=Test pod already exists"
        else
          echo "skip_test=false"
          echo "reason=Ready to test"
        fi
      register: test_check
      changed_when: false

    - name: Run CUDA test
      block:
        - name: Create CUDA test manifest
          ansible.builtin.copy:
            content: "{{ cuda_test_pod }}"
            dest: "{{ cuda_test_manifest }}"
            mode: '0600'

        - name: Deploy CUDA test pod
          ansible.builtin.command: "{{ kubectl_bin }} apply -f {{ cuda_test_manifest }}"
          register: deploy_result
          changed_when: "'created' in deploy_result.stdout"

        - name: Wait for CUDA test pod completion
          ansible.builtin.shell: |
            i=1
            while [ $i -le 60 ]; do
              phase=$({{ kubectl_bin }} get pod cuda-vectoradd -n {{ cuda_test_namespace }} -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")
              
              if [ "$phase" = "Succeeded" ]; then
                echo "âœ“ CUDA test completed successfully"
                exit 0
              elif [ "$phase" = "Failed" ] || [ "$phase" = "Error" ]; then
                echo "âœ— CUDA test failed with status: $phase"
                # Get pod events for debugging
                echo "Pod events:"
                {{ kubectl_bin }} describe pod cuda-vectoradd -n {{ cuda_test_namespace }} | grep -A5 Events || true
                exit 1
              fi
              
              echo "Waiting for CUDA test pod... (attempt $i/60, status: $phase)"
              sleep 5
              i=$((i + 1))
            done
            
            echo "âœ— CUDA test pod did not complete in time"
            # Get final pod status
            echo "Final pod status:"
            {{ kubectl_bin }} describe pod cuda-vectoradd -n {{ cuda_test_namespace }} || true
            exit 1
          register: cuda_test
          changed_when: false
          failed_when: cuda_test.rc != 0

        - name: Get CUDA test pod logs
          ansible.builtin.command: "{{ kubectl_bin }} logs cuda-vectoradd -n {{ cuda_test_namespace }}"
          register: cuda_logs
          changed_when: false

        - name: Display CUDA test results
          ansible.builtin.debug:
            var: cuda_logs.stdout_lines

        - name: Clean up CUDA test pod
          ansible.builtin.command: "{{ kubectl_bin }} delete -f {{ cuda_test_manifest }}"
          register: delete_result
          changed_when: "'deleted' in delete_result.stdout"
          failed_when: false

        - name: Remove temporary files
          ansible.builtin.file:
            path: "{{ cuda_test_manifest }}"
            state: absent
      when: "'skip_test=false' in test_check.stdout"

  post_tasks:
    - name: Verify GPU operator status
      ansible.builtin.shell: |
        echo "=== GPU Operator Deployment Summary ==="
        echo ""
        echo "GPU-enabled nodes:"
        {{ kubectl_bin }} get nodes -o custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu,GPU-PRODUCT:.metadata.labels.nvidia\\.com/gpu\\.product
        echo ""
        echo "GPU operator pods:"
        {{ kubectl_bin }} get pods -n {{ gpu_operator_namespace }} --no-headers | awk '{print $1 "\t" $3}' | column -t
        echo ""
        # Check if GPUs are available
        gpu_count=$({{ kubectl_bin }} get nodes -o json | jq '[.items[].status.allocatable."nvidia.com/gpu" // "0" | tonumber] | add')
        if [ "$gpu_count" -gt 0 ]; then
          echo "âœ“ Total GPUs available in cluster: $gpu_count"
          exit 0
        else
          echo "âœ— No GPUs detected in cluster"
          exit 1
        fi
      register: gpu_summary
      changed_when: false
      failed_when: gpu_summary.rc != 0

    - name: Display deployment summary
      ansible.builtin.debug:
        var: gpu_summary.stdout_lines