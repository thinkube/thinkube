# Copyright 2025 Alejandro Mart√≠nez Corri√° and the Thinkube contributors
# SPDX-License-Identifier: Apache-2.0

---
# ansible/40_thinkube/core/infrastructure/k8s/10_install_k8s.yaml
# Description:
#   Installs and configures Canonical Kubernetes (via k8s-snap) on control plane node
#   - Installs k8s snap package
#   - Installs kubectl and helm binaries (no sudo required)
#   - Creates k8s-hostpath StorageClass (alias to csi-rawfile-default)
#   - Enables Cilium load balancer (built-in replacement for MetalLB)
#
# Requirements:
#   - Ubuntu 24.04 LTS
#   - 16 cores minimum
#   - 64GB RAM minimum
#   - 1TB disk minimum
#   - UFW firewall enabled
#
# Usage:
#   cd ~/thinkube
#   ./scripts/run_ansible.sh ansible/40_thinkube/core/infrastructure/k8s/10_install_k8s.yaml
#
# Variables from inventory:
#   - system_username: User to install kubectl/helm for
#   - domain_name: Domain name for services
#   - zerotier_subnet_prefix: ZeroTier network subnet prefix
#
# ü§ñ [AI-assisted]

- name: Install Canonical Kubernetes Control Plane
  hosts: k8s_control_plane
  become: false
  gather_facts: true

  vars:
    k8s_channel: "1.34-classic/stable"
    k8s_version: "1.34.0"
    user: "{{ system_username }}"
    user_home: "/home/{{ user }}"
    kubectl_wrapper_path: "{{ user_home }}/.local/bin/kubectl"
    helm_wrapper_path: "{{ user_home }}/.local/bin/helm"

  pre_tasks:
    - name: Verify required variables exist
      ansible.builtin.fail:
        msg: "Required variable {{ item }} is not defined"
      when: vars[item] is not defined
      loop:
        - system_username
        - domain_name
        - zerotier_subnet_prefix

    - name: Verify OS is Ubuntu 24.04
      ansible.builtin.fail:
        msg: "This playbook requires Ubuntu 24.04 LTS (found {{ ansible_distribution }} {{ ansible_distribution_version }})"
      when: ansible_distribution != 'Ubuntu' or ansible_distribution_version != '24.04'

    - name: Verify minimum CPU cores
      ansible.builtin.fail:
        msg: "Minimum 16 CPU cores required (found {{ ansible_processor_vcpus }})"
      when: ansible_processor_vcpus | int < 16

    - name: Verify minimum RAM
      ansible.builtin.fail:
        msg: "Minimum 64GB RAM required (found {{ (ansible_memtotal_mb / 1024) | round(1) }}GB)"
      when: (ansible_memtotal_mb / 1024) | int < 64

  tasks:
    # ===========================
    # DGX Spark Specific: Docker Coexistence
    # ===========================
    # Note: DGX Spark ships with Docker pre-installed and enabled.
    # We configure k8s-snap to use a custom containerd base directory (/var/lib/k8s-containerd)
    # to allow Docker and k8s-snap to coexist without conflicts.
    # This enables users to use Docker for other DGX Spark capabilities while running Kubernetes.
    #
    # Docker uses: /run/containerd, /var/lib/containerd
    # k8s-snap uses: /var/lib/k8s-containerd (custom base directory)

    # ===========================
    # UFW Firewall Configuration (CRITICAL for CoreDNS)
    # ===========================
    - name: Ensure UFW is installed
      ansible.builtin.apt:
        name: ufw
        state: present
        update_cache: true
      become: true

    - name: Configure UFW default forward policy to ACCEPT (required for k8s networking)
      ansible.builtin.lineinfile:
        path: /etc/default/ufw
        regexp: '^DEFAULT_FORWARD_POLICY='
        line: 'DEFAULT_FORWARD_POLICY="ACCEPT"'
        backup: true
      become: true
      notify: reload_ufw

    - name: Allow k8s API server port (6443/tcp)
      community.general.ufw:
        rule: allow
        port: '6443'
        proto: tcp
        comment: 'k8s API server'
      become: true

    - name: Allow k8s cluster daemon port (6400/tcp)
      community.general.ufw:
        rule: allow
        port: '6400'
        proto: tcp
        comment: 'k8s cluster daemon'
      become: true

    - name: Allow kubelet API port (10250/tcp)
      community.general.ufw:
        rule: allow
        port: '10250'
        proto: tcp
        comment: 'k8s kubelet'
      become: true

    - name: Allow Cilium health port (4240/tcp)
      community.general.ufw:
        rule: allow
        port: '4240'
        proto: tcp
        comment: 'Cilium networking'
      become: true

    - name: Allow Cilium VXLAN port (8472/udp)
      community.general.ufw:
        rule: allow
        port: '8472'
        proto: udp
        comment: 'Cilium VXLAN'
      become: true

    - name: Allow traffic on cilium_host interface
      community.general.ufw:
        rule: allow
        interface: cilium_host
        direction: in
      become: true

    - name: Allow outbound traffic on cilium_host interface
      community.general.ufw:
        rule: allow
        interface: cilium_host
        direction: out
      become: true

    - name: Enable UFW
      community.general.ufw:
        state: enabled
      become: true

    - name: Reload UFW to apply forward policy
      ansible.builtin.command: ufw reload
      become: true
      changed_when: true
      when: ufw_forward_policy_changed | default(false)

    # ===========================
    # k8s-snap Installation
    # ===========================
    - name: Check if k8s-snap is already installed
      ansible.builtin.command: snap list k8s
      become: true
      register: k8s_snap_check
      failed_when: false
      changed_when: false

    - name: Install k8s-snap
      community.general.snap:
        name: k8s
        classic: true
        channel: "{{ k8s_channel }}"
        state: present
      become: true
      when: k8s_snap_check.rc != 0

    - name: Create k8s bootstrap configuration with custom containerd base directory
      ansible.builtin.copy:
        content: |
          containerd-base-dir: /var/lib/k8s-containerd
          cluster-config:
            network:
              enabled: true
            dns:
              enabled: true
            local-storage:
              enabled: true
          extra-node-kubelet-args:
            --max-pods: "500"
            --root-dir: "/var/snap/k8s/common/var/lib/kubelet"
        dest: /tmp/k8s-bootstrap-config.yaml
        mode: '0644'
      become: true

    - name: Bootstrap k8s cluster with custom containerd base directory
      ansible.builtin.shell: |
        k8s bootstrap --file /tmp/k8s-bootstrap-config.yaml
      become: true
      register: bootstrap_result
      changed_when: "'Bootstrapped' in bootstrap_result.stdout"
      failed_when:
        - bootstrap_result.rc != 0
        - "'already bootstrapped' not in bootstrap_result.stderr"
        - "'already part of a cluster' not in bootstrap_result.stderr"

    - name: Wait for k8s cluster to be ready
      ansible.builtin.command: k8s status --wait-ready
      become: true
      register: k8s_status
      retries: 30
      delay: 10
      until: k8s_status.rc == 0
      changed_when: false

    - name: Verify cluster status
      ansible.builtin.command: k8s status
      become: true
      register: cluster_status
      changed_when: false

    - name: Display cluster status
      ansible.builtin.debug:
        var: cluster_status.stdout_lines

    # ===========================
    # Configure containerd for GPU operator compatibility
    # ===========================
    # Create a base runc runtime config that ensures k8s-snap's runc runtime persists
    # even when GPU operator adds nvidia runtime via imported configs.
    # This file is imported BEFORE any GPU operator configs (00- prefix).
    - name: Create containerd config directory
      ansible.builtin.file:
        path: /etc/containerd/conf.d
        state: directory
        mode: '0755'
      become: true

    - name: Create k8s-snap runc runtime config for GPU compatibility
      ansible.builtin.copy:
        content: |
          version = 2

          [plugins]
            [plugins."io.containerd.grpc.v1.cri"]
              [plugins."io.containerd.grpc.v1.cri".containerd]
                default_runtime_name = "runc"

                [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
                  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
                    runtime_type = "io.containerd.runc.v2"
                    [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
                      SystemdCgroup = true
        dest: /etc/containerd/conf.d/00-k8s-runc.toml
        mode: '0644'
      become: true

    - name: Signal k8s-snap containerd to reload config
      ansible.builtin.command: pkill -SIGHUP containerd
      become: true
      register: containerd_reload
      failed_when: false
      changed_when: containerd_reload.rc == 0

    # ===========================
    # Verify Core Components
    # ===========================
    - name: Wait for CoreDNS to be ready
      ansible.builtin.command: k8s kubectl wait --for=condition=ready pod -l k8s-app=coredns -n kube-system --timeout=300s
      become: true
      register: coredns_wait
      retries: 3
      delay: 10
      until: coredns_wait.rc == 0
      changed_when: false

    - name: Wait for Cilium to be ready
      ansible.builtin.command: k8s kubectl wait --for=condition=ready pod -l k8s-app=cilium -n kube-system --timeout=300s
      become: true
      register: cilium_wait
      retries: 3
      delay: 10
      until: cilium_wait.rc == 0
      changed_when: false

    - name: Verify node is ready
      ansible.builtin.command: k8s kubectl get nodes
      become: true
      register: nodes_status
      changed_when: false

    - name: Check that node is in Ready state
      ansible.builtin.assert:
        that:
          - "'Ready' in nodes_status.stdout"
        fail_msg: "Node is not in Ready state"
        success_msg: "Node is Ready"

    # ===========================
    # Verify Storage Class
    # ===========================
    - name: Get storage classes
      ansible.builtin.command: k8s kubectl get storageclass
      become: true
      register: storage_classes
      changed_when: false

    - name: Verify csi-rawfile-default storage class exists
      ansible.builtin.assert:
        that:
          - "'csi-rawfile-default' in storage_classes.stdout"
        fail_msg: "csi-rawfile-default storage class not found"
        success_msg: "Storage class csi-rawfile-default is available"

    # ===========================
    # Patch rawfile CSI driver for custom kubelet root-dir
    # ===========================
    - name: Patch rawfile CSI node daemonset to use correct kubelet paths
      ansible.builtin.shell: |
        k8s kubectl patch daemonset ck-storage-rawfile-csi-node -n kube-system --type='json' -p='[
          {
            "op": "replace",
            "path": "/spec/template/spec/volumes/0/hostPath/path",
            "value": "/var/snap/k8s/common/var/lib/kubelet/plugins_registry"
          },
          {
            "op": "replace",
            "path": "/spec/template/spec/volumes/1/hostPath/path",
            "value": "/var/snap/k8s/common/var/lib/kubelet/plugins/rawfile-csi"
          },
          {
            "op": "replace",
            "path": "/spec/template/spec/volumes/2/hostPath/path",
            "value": "/var/snap/k8s/common/var/lib/kubelet"
          },
          {
            "op": "replace",
            "path": "/spec/template/spec/containers/0/volumeMounts/1/mountPath",
            "value": "/var/snap/k8s/common/var/lib/kubelet"
          },
          {
            "op": "replace",
            "path": "/spec/template/spec/containers/1/env/1/value",
            "value": "/var/snap/k8s/common/var/lib/kubelet/plugins/rawfile-csi/csi.sock"
          }
        ]'
      become: true
      register: csi_patch_result
      changed_when: csi_patch_result.rc == 0
      failed_when: false

    - name: Wait for rawfile CSI node pods to be ready after patch
      ansible.builtin.command: "k8s kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=rawfile-csi -n kube-system --timeout=300s"
      become: true
      register: rawfile_csi_wait
      retries: 3
      delay: 10
      until: rawfile_csi_wait.rc == 0
      changed_when: false

    # ===========================
    # Install kubectl and helm Binaries (No Sudo Required)
    # ===========================
    - name: Create .local/bin directory for user
      ansible.builtin.file:
        path: "{{ user_home }}/.local/bin"
        state: directory
        owner: "{{ user }}"
        group: "{{ user }}"
        mode: '0755'
      become_user: "{{ user }}"

    - name: Detect system architecture
      ansible.builtin.set_fact:
        system_arch: "{{ 'arm64' if ansible_architecture == 'aarch64' else 'amd64' }}"

    - name: Download kubectl binary
      ansible.builtin.get_url:
        url: "https://dl.k8s.io/release/v{{ k8s_version }}/bin/linux/{{ system_arch }}/kubectl"
        dest: "{{ kubectl_wrapper_path }}"
        mode: '0755'
        owner: "{{ user }}"
        group: "{{ user }}"
      become_user: "{{ user }}"

    - name: Download helm installer
      ansible.builtin.get_url:
        url: "https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3"
        dest: "/tmp/get-helm-3.sh"
        mode: '0755'
        force: true
      become: true

    - name: Install helm binary
      ansible.builtin.shell: |
        export HELM_INSTALL_DIR="{{ user_home }}/.local/bin"
        export USE_SUDO=false
        /tmp/get-helm-3.sh
      args:
        creates: "{{ helm_wrapper_path }}"
      environment:
        PATH: "{{ user_home }}/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
      become_user: "{{ user }}"

    - name: Set helm binary ownership
      ansible.builtin.file:
        path: "{{ helm_wrapper_path }}"
        owner: "{{ user }}"
        group: "{{ user }}"
        mode: '0755'
      become: true

    # ===========================
    # Create kubeconfig for user
    # ===========================
    - name: Create .kube directory
      ansible.builtin.file:
        path: "{{ user_home }}/.kube"
        state: directory
        owner: "{{ user }}"
        group: "{{ user }}"
        mode: '0755'
      become: false

    - name: Create .kube directory
      ansible.builtin.file:
        path: "{{ user_home }}/.kube"
        state: directory
        owner: "{{ user }}"
        group: "{{ user }}"
        mode: '0755'
      become: false

    - name: Copy kubeconfig from k8s-snap
      ansible.builtin.copy:
        src: /etc/kubernetes/admin.conf
        dest: "{{ user_home }}/.kube/config"
        remote_src: true
        owner: "{{ user }}"
        group: "{{ user }}"
        mode: '0600'
      become: true

    # ===========================
    # Deploy k8s-hostpath StorageClass
    # ===========================
    - name: Copy k8s-hostpath StorageClass manifest
      ansible.builtin.copy:
        src: files/k8s-hostpath-storageclass.yaml
        dest: /tmp/k8s-hostpath-storageclass.yaml
        mode: '0644'

    - name: Deploy k8s-hostpath StorageClass
      ansible.builtin.shell: |
        k8s kubectl apply -f /tmp/k8s-hostpath-storageclass.yaml
      become: true
      register: storageclass_result
      changed_when: "'created' in storageclass_result.stdout or 'configured' in storageclass_result.stdout"

    - name: Verify k8s-hostpath StorageClass exists
      ansible.builtin.command: k8s kubectl get storageclass k8s-hostpath
      become: true
      register: k8s_hostpath_sc
      changed_when: false

    - name: Display StorageClass status
      ansible.builtin.debug:
        msg: "k8s-hostpath StorageClass deployed successfully (using rawfile.csi.openebs.io provisioner)"

    # ===========================
    # Thinkube Shared Shell Alias System Integration
    # ===========================
    - name: Check if thinkube_shared_shell directory exists
      ansible.builtin.stat:
        path: "{{ user_home }}/.thinkube_shared_shell"
      register: shared_shell_dir

    - name: Integrate kubectl aliases into thinkube_shared_shell
      when: shared_shell_dir.stat.exists
      block:
        - name: Ensure aliases directory exists
          ansible.builtin.file:
            path: "{{ user_home }}/.thinkube_shared_shell/aliases"
            state: directory
            owner: "{{ user }}"
            group: "{{ user }}"
            mode: '0755'

        - name: Create kubectl aliases JSON file
          ansible.builtin.copy:
            content: |
              {
                "k": "kubectl",
                "kg": "kubectl get",
                "kd": "kubectl describe",
                "kdel": "kubectl delete",
                "kl": "kubectl logs",
                "ke": "kubectl edit",
                "kex": "kubectl exec -it",
                "kgp": "kubectl get pods",
                "kgn": "kubectl get nodes",
                "kgs": "kubectl get svc",
                "kgd": "kubectl get deployments",
                "kga": "kubectl get all",
                "kgpa": "kubectl get pods -A",
                "kgna": "kubectl get nodes -o wide",
                "kaf": "kubectl apply -f",
                "kdf": "kubectl delete -f"
              }
            dest: "{{ user_home }}/.thinkube_shared_shell/aliases/kubectl_aliases.json"
            owner: "{{ user }}"
            group: "{{ user }}"
            mode: '0644'

        - name: Create helm aliases JSON file
          ansible.builtin.copy:
            content: |
              {
                "h": "helm",
                "hl": "helm list",
                "hla": "helm list -A",
                "hi": "helm install",
                "hu": "helm upgrade",
                "hd": "helm delete",
                "hs": "helm status",
                "hh": "helm history"
              }
            dest: "{{ user_home }}/.thinkube_shared_shell/aliases/helm_aliases.json"
            owner: "{{ user }}"
            group: "{{ user }}"
            mode: '0644'

        - name: Create k8s aliases shell script for bash/zsh
          ansible.builtin.copy:
            content: |
              #!/bin/bash
              # k8s-snap kubectl and helm aliases for bash/zsh
              # Source this file in your shell rc file

              # kubectl aliases
              alias k='kubectl'
              alias kg='kubectl get'
              alias kd='kubectl describe'
              alias kdel='kubectl delete'
              alias kl='kubectl logs'
              alias ke='kubectl edit'
              alias kex='kubectl exec -it'
              alias kgp='kubectl get pods'
              alias kgn='kubectl get nodes'
              alias kgs='kubectl get svc'
              alias kgd='kubectl get deployments'
              alias kga='kubectl get all'
              alias kgpa='kubectl get pods -A'
              alias kgna='kubectl get nodes -o wide'
              alias kaf='kubectl apply -f'
              alias kdf='kubectl delete -f'

              # helm aliases
              alias h='helm'
              alias hl='helm list'
              alias hla='helm list -A'
              alias hi='helm install'
              alias hu='helm upgrade'
              alias hd='helm delete'
              alias hs='helm status'
              alias hh='helm history'
            dest: "{{ user_home }}/.thinkube_shared_shell/aliases/k8s_aliases.sh"
            owner: "{{ user }}"
            group: "{{ user }}"
            mode: '0644'

        - name: Create k8s aliases for fish shell
          ansible.builtin.copy:
            content: |
              # k8s-snap kubectl and helm aliases for fish
              # Source this file in your fish config

              # kubectl aliases
              abbr -a k kubectl
              abbr -a kg kubectl get
              abbr -a kd kubectl describe
              abbr -a kdel kubectl delete
              abbr -a kl kubectl logs
              abbr -a ke kubectl edit
              abbr -a kex kubectl exec -it
              abbr -a kgp kubectl get pods
              abbr -a kgn kubectl get nodes
              abbr -a kgs kubectl get svc
              abbr -a kgd kubectl get deployments
              abbr -a kga kubectl get all
              abbr -a kgpa kubectl get pods -A
              abbr -a kgna kubectl get nodes -o wide
              abbr -a kaf kubectl apply -f
              abbr -a kdf kubectl delete -f

              # helm aliases
              abbr -a h helm
              abbr -a hl helm list
              abbr -a hla helm list -A
              abbr -a hi helm install
              abbr -a hu helm upgrade
              abbr -a hd helm delete
              abbr -a hs helm status
              abbr -a hh helm history
            dest: "{{ user_home }}/.thinkube_shared_shell/aliases/k8s_aliases.fish"
            owner: "{{ user }}"
            group: "{{ user }}"
            mode: '0644'

    # ===========================
    # Cilium Load Balancer (replaces MetalLB)
    # ===========================
    - name: Build load balancer IP range variables
      ansible.builtin.set_fact:
        lb_ip_range_start: "{{ zerotier_subnet_prefix }}{{ metallb_ip_start_octet }}"
        lb_ip_range_end: "{{ zerotier_subnet_prefix }}{{ metallb_ip_end_octet }}"

    - name: Enable Cilium load balancer
      ansible.builtin.command: k8s enable load-balancer
      become: true
      register: lb_enable
      changed_when: "'Enabled' in lb_enable.stdout or 'enabled' in lb_enable.stdout"

    - name: Configure Cilium load balancer IP range and L2 mode
      ansible.builtin.command: k8s set load-balancer.cidrs="{{ lb_ip_range_start }}-{{ lb_ip_range_end }}" load-balancer.l2-mode=true
      become: true
      register: lb_config
      changed_when: true

    - name: Wait for load balancer to be ready
      ansible.builtin.pause:
        seconds: 10
        prompt: "Waiting for Cilium load balancer to initialize..."

    - name: Show load balancer status
      ansible.builtin.debug:
        msg: "Cilium load balancer is configured with IP range {{ lb_ip_range_start }}-{{ lb_ip_range_end }}"

  handlers:
    - name: reload_ufw
      ansible.builtin.shell: |
        ufw disable
        ufw --force enable
      become: true
      listen: reload_ufw
      changed_when: true

  post_tasks:
    - name: Display installation summary
      ansible.builtin.debug:
        msg:
          - "==============================================="
          - "Canonical Kubernetes (k8s-snap) Installation Complete"
          - "==============================================="
          - "Version: {{ k8s_version }}"
          - "Channel: {{ k8s_channel }}"
          - "Node Status: {{ 'Ready' if 'Ready' in nodes_status.stdout else 'Not Ready' }}"
          - "CoreDNS: {{ 'Running' if coredns_wait.rc == 0 else 'Not Ready' }}"
          - "Cilium CNI: {{ 'Running' if cilium_wait.rc == 0 else 'Not Ready' }}"
          - "Storage Classes: csi-rawfile-default (default), k8s-hostpath"
          - "kubectl: Installed at {{ kubectl_wrapper_path }} (no sudo required)"
          - "helm: Installed at {{ helm_wrapper_path }} (no sudo required)"
          - "==============================================="
          - "Next Steps:"
          - "1. Join worker nodes: ansible/40_thinkube/core/infrastructure/k8s/20_join_workers.yaml"
          - "2. Install GPU Operator: ansible/40_thinkube/core/infrastructure/gpu_operator/10_deploy.yaml"
          - "==============================================="
