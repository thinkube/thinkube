# Copyright 2025 Alejandro Martínez Corriá and the Thinkube contributors
# SPDX-License-Identifier: Apache-2.0

# JupyterHub Helm chart values with dynamic image discovery
# Images discovered at runtime from thinkube-control API
# No fallbacks - fails if dependencies unavailable

hub:
  config:
    JupyterHub:
      # MANDATORY: Keycloak authentication only - no fallbacks
      authenticator_class: generic-oauth
      admin_access: true
      admin_users:
        - {{ admin_username }}
      allow_named_servers: false
      shutdown_on_logout: true

      # Thinkube AI Lab branding
      template_vars:
        announcement: "Welcome to Thinkube AI Lab - Thinkube's intelligent notebook laboratory"

    # Keycloak authentication is mandatory
    GenericOAuthenticator:
      client_id: jupyterhub
      client_secret: "{{ jupyterhub_client_secret }}"
      oauth_callback_url: "https://jupyter.{{ domain_name }}/hub/oauth_callback"
      authorize_url: "https://auth.{{ domain_name }}/realms/{{ keycloak_realm }}/protocol/openid-connect/auth"
      token_url: "https://auth.{{ domain_name }}/realms/{{ keycloak_realm }}/protocol/openid-connect/token"
      userdata_url: "https://auth.{{ domain_name }}/realms/{{ keycloak_realm }}/protocol/openid-connect/userinfo"
      logout_redirect_url: "https://auth.{{ domain_name }}/realms/{{ keycloak_realm }}/protocol/openid-connect/logout?redirect_uri=https://control.{{ domain_name }}"
      login_service: "Keycloak"
      username_claim: "preferred_username"
      scope:
        - "openid"
        - "profile"
        - "email"
      allow_all: true

  extraConfig:
    00-spawner-config: |
      # Set default URL to JupyterLab opening in notebooks folder
      c.Spawner.default_url = '/lab/tree/thinkube/notebooks'

      # Configure KubeSpawner for faster pod deletion
      # Based on official KubeSpawner documentation
      c.KubeSpawner.delete_grace_period = 1  # Grace period for pod deletion
      c.KubeSpawner.delete_stopped_pods = True  # Delete pods when stopped (default)

      # Set terminationGracePeriodSeconds via extra_pod_config
      # This controls how long Kubernetes waits before forcefully killing the pod
      c.KubeSpawner.extra_pod_config = {
          'terminationGracePeriodSeconds': 1
      }

      # Reduce spawn/stop timeouts
      c.Spawner.http_timeout = 30  # Reduce from default 60
      c.Spawner.start_timeout = 120  # Reduce from default 300
      c.Spawner.stop_timeout = 10  # Reduce from default 60 - time to wait for stop

      # Set KUBECONFIG as environment variable for notebook kernels
      # Use update() to merge with extraEnv rather than overwriting
      c.KubeSpawner.environment.update({
        'KUBECONFIG': '/home/thinkube/.config/thinkube/kube-config'
      })

      # Disable the native python3 kernel - we provide our own venv kernels
      c.KernelSpecManager.ensure_native_kernel = False

      # Override command to source service discovery environment
      import textwrap
      c.KubeSpawner.cmd = ['bash', '-c', textwrap.dedent("""
      if [ -f /home/thinkube/.config/thinkube/service-env-jh.sh ]; then
        echo "Loading service environment variables..."
        source /home/thinkube/.config/thinkube/service-env-jh.sh
      fi
      exec jupyterhub-singleuser --ServerApp.trust_xheaders=True --KernelSpecManager.ensure_native_kernel=False
      """).strip()]

    01-dynamic-profile-generator: |
      # Dynamic profile generation with flexible resource selection
      import requests
      import sys
      import logging

      def parse_memory_to_gb(memory_str):
          """Convert memory string to GB for comparison"""
          if memory_str.endswith('Gi'):
              return int(memory_str[:-2])
          elif memory_str.endswith('G'):
              return int(memory_str[:-1])
          elif memory_str.endswith('Mi'):
              return int(memory_str[:-2]) / 1024
          elif memory_str.endswith('M'):
              return int(memory_str[:-1]) / 1024
          return 0

      def get_profile_list(spawner):
          """Generate profiles with flexible resource selection forms"""
          logging.basicConfig(level=logging.INFO)
          logger = logging.getLogger(__name__)

          try:
              # Query thinkube-control for available resources
              logger.info("Querying thinkube-control for cluster resources...")
              resources_response = requests.get(
                  'http://backend.thinkube-control.svc.cluster.local:8000/api/v1/cluster/resources',
                  headers={'Accept': 'application/json'},
                  timeout=10
              )

              if resources_response.status_code != 200:
                  logger.error(f"Failed to get cluster resources: {resources_response.status_code}")
                  sys.exit(1)

              resources = resources_response.json()

              # Query for JupyterHub configuration (defaults)
              logger.info("Querying thinkube-control for JupyterHub configuration...")
              config_response = requests.get(
                  'http://backend.thinkube-control.svc.cluster.local:8000/api/v1/jupyterhub/config',
                  headers={'Accept': 'application/json'},
                  timeout=10
              )

              if config_response.status_code != 200:
                  logger.error(f"Failed to get JupyterHub config: {config_response.status_code}")
                  logger.error("Cannot proceed without JupyterHub configuration from thinkube-control")
                  sys.exit(1)

              jupyterhub_config = config_response.json()
              logger.info(f"Loaded JupyterHub config: node={jupyterhub_config.get('default_node')}, defaults=({jupyterhub_config['default_cpu_cores']}CPU, {jupyterhub_config['default_memory_gb']}GB, {jupyterhub_config['default_gpu_count']}GPU)")

              profiles = []

              # Find max capacity across all nodes (users should see full capacity, not just available)
              max_cpu = max(node['capacity']['cpu'] for node in resources)
              max_memory_gb = max(parse_memory_to_gb(node['capacity']['memory']) for node in resources)
              has_gpu = any(node['available']['gpu'] > 0 for node in resources)

              # Build dynamic CPU choices based on capacity
              cpu_choices = {}
              cpu_options = [1, 2, 4, 6, 8, 12, 16, 24, 32]

              # Ensure default is an integer and add if not in standard options
              default_cpu = int(jupyterhub_config['default_cpu_cores'])
              if default_cpu not in cpu_options:
                  cpu_options.append(default_cpu)
                  cpu_options.sort()

              # Find best default if configured default exceeds capacity
              actual_default = default_cpu if default_cpu <= max_cpu else max(c for c in cpu_options if c <= max_cpu)
              logger.info(f"CPU: configured default={default_cpu}, max_cpu={max_cpu}, actual_default={actual_default}")

              for cores in cpu_options:
                  if cores <= max_cpu:
                      cpu_choices[str(cores)] = {
                          'display_name': f"{cores} core{'s' if cores > 1 else ''}",
                          'default': (cores == actual_default)
                      }

              # Build dynamic memory choices based on capacity
              memory_choices = {}
              memory_options = [2, 4, 8, 16, 32, 48, 64, 96, 128]

              # Ensure default is an integer and add if not in standard options
              default_memory = int(jupyterhub_config['default_memory_gb'])
              if default_memory not in memory_options:
                  memory_options.append(default_memory)
                  memory_options.sort()

              # Find best default if configured default exceeds capacity
              actual_default_memory = default_memory if default_memory <= max_memory_gb else max(m for m in memory_options if m <= max_memory_gb)
              logger.info(f"Memory: configured default={default_memory}GB, max_memory={max_memory_gb}GB, actual_default={actual_default_memory}GB")

              for gb in memory_options:
                  if gb <= max_memory_gb:
                      memory_choices[f"{gb}G"] = {
                          'display_name': f"{gb} GB",
                          'default': (gb == actual_default_memory)
                      }

              # Build node choices with resource info (shared across all profiles)
              node_choices = {}
              for node in resources:
                  node_label = (f"{node['name']} "
                              f"({node['available']['cpu']:.1f}/{node['capacity']['cpu']} CPUs, "
                              f"{node['available']['memory']}/{node['capacity']['memory']}")

                  # Add GPU info if available
                  if node['gpu_details']:
                      available_gpus = [g for g in node['gpu_details'] if g['available']]
                      if available_gpus:
                          gpu = available_gpus[0]
                          node_label += f", GPU: {gpu['model']} {gpu['memory_total']}"
                      else:
                          node_label += ", GPU: In use"
                  else:
                      node_label += ", No GPU"

                  node_label += ")"
                  node_choices[node['name']] = {
                      'display_name': node_label,
                      'default': (node['name'] == jupyterhub_config.get('default_node'))
                  }

              # Build GPU choices based on max available GPUs across nodes
              max_gpus = max(node['available']['gpu'] for node in resources)
              gpu_choices = {}
              for i in range(0, max_gpus + 1):
                  gpu_choices[str(i)] = {
                      'display_name': f'{i} GPU{"s" if i > 1 else ""}' if i > 0 else 'No GPU',
                      'default': (i == jupyterhub_config['default_gpu_count'])
                  }

              # Create a single "Resource Selection" profile
              # Image is fixed to tk-jupyter-base (venvs provide different Python environments)
              profile = {
                  'display_name': 'Thinkube AI Lab',
                  'description': 'Select compute resources (Python environments available via kernel selection)',
                  'default': True,
                  'profile_options': {
                      'node': {
                          'display_name': 'Select Node',
                          'choices': node_choices
                      },
                      'cpu': {
                          'display_name': 'CPU Cores',
                          'choices': cpu_choices
                      },
                      'memory': {
                          'display_name': 'Memory',
                          'choices': memory_choices
                      },
                      'enable_gpu': {
                          'display_name': 'Number of GPUs',
                          'choices': gpu_choices
                      }
                  },
                  'kubespawner_override': {
                      'image': '{{ harbor_registry }}/library/tk-jupyter-base:latest',
                      'image_pull_policy': 'Always'
                  }
              }
              profiles.append(profile)

              # Debug logging to verify defaults
              default_node = next((k for k, v in node_choices.items() if v.get('default')), None)
              default_cpu = next((k for k, v in cpu_choices.items() if v.get('default')), None)
              default_memory = next((k for k, v in memory_choices.items() if v.get('default')), None)
              default_gpu = next((k for k, v in gpu_choices.items() if v.get('default')), None)
              logger.info(f"Profile defaults set to:")
              logger.info(f"  image: tk-jupyter-base (fixed)")
              logger.info(f"  node: {default_node}")
              logger.info(f"  cpu: {default_cpu}")
              logger.info(f"  memory: {default_memory}")
              logger.info(f"  gpu: {default_gpu}")

              if not profiles:
                  logger.error("No profiles could be generated")
                  sys.exit(1)

              logger.info(f"Successfully generated {len(profiles)} profiles with resource options")
              return profiles

          except Exception as e:
              logger.error(f"Failed to generate profiles: {e}")
              sys.exit(1)

      # Apply resource selections using pre_spawn_hook
      async def resource_selector_hook(spawner):
          """Apply resource selections from profile_options"""
          import logging
          import requests
          logger = logging.getLogger(__name__)

          # Load user options first as per documentation
          await spawner.load_user_options()

          user_options = spawner.user_options
          logger.info(f"Pre-spawn hook - user_options: {user_options}")

          # Log the exact structure we're getting
          if isinstance(user_options, dict):
              logger.info(f"user_options keys: {user_options.keys()}")
              if 'profile' in user_options:
                  logger.info(f"profile value type: {type(user_options['profile'])}")
                  logger.info(f"profile value: {user_options['profile']}")

          # Get defaults from thinkube-control config
          defaults = {'cpu': '4', 'memory': '8G', 'gpu': '0'}
          try:
              config_response = requests.get(
                  'http://backend.thinkube-control.svc.cluster.local:8000/api/v1/jupyterhub/config',
                  timeout=5
              )
              if config_response.status_code == 200:
                  config = config_response.json()
                  defaults['cpu'] = str(config['default_cpu_cores'])
                  defaults['memory'] = f"{config['default_memory_gb']}G"
                  defaults['gpu'] = str(config['default_gpu_count'])
                  logger.info(f"Loaded defaults from thinkube-control: CPU={defaults['cpu']}, Memory={defaults['memory']}, GPU={defaults['gpu']}")
          except Exception as e:
              logger.warning(f"Could not load defaults from thinkube-control, using fallbacks: {e}")

          # When using profile_options, the selected values are directly in user_options
          if user_options:
              # Apply node selection
              node = user_options.get('node')
              if node:
                  spawner.node_selector = {"kubernetes.io/hostname": node}
                  logger.info(f"Set node selector: {node}")

              # Apply CPU and memory (use thinkube-control defaults as fallback)
              cpu = user_options.get('cpu', defaults['cpu'])
              spawner.cpu_limit = float(cpu)
              spawner.cpu_guarantee = float(cpu) * 0.5
              logger.info(f"Set CPU: {cpu}")

              memory = user_options.get('memory', defaults['memory'])
              spawner.mem_limit = memory
              spawner.mem_guarantee = memory
              logger.info(f"Set memory: {memory}")

              # Apply GPU allocation
              gpu_count = user_options.get('enable_gpu', defaults['gpu'])
              if gpu_count != '0':
                  spawner.extra_resource_limits = {"nvidia.com/gpu": gpu_count}
                  spawner.extra_resource_guarantees = {"nvidia.com/gpu": gpu_count}
                  logger.info(f"Set GPU count: {gpu_count}")

      c.KubeSpawner.profile_list = get_profile_list
      c.KubeSpawner.pre_spawn_hook = resource_selector_hook

    02-thinkube-branding: |
      # Thinkube branding - logo and custom templates
      c.JupyterHub.logo_file = '/usr/local/share/jupyterhub/static/images/tk_ai.svg'

      # Custom template directory (searched before default templates)
      c.JupyterHub.template_paths = ['/usr/local/share/jupyterhub/custom-templates']

  # Network policy configuration
  # Disable default network policy - we'll create a custom one that allows ingress controller access
  networkPolicy:
    enabled: false

  # Thinkube AI Lab branding assets
  extraFiles:
    tk-ai-logo:
      mountPath: /usr/local/share/jupyterhub/static/images/tk_ai.svg
      stringData: |
        {{ lookup('file', playbook_dir + '/templates/tk_ai.svg') | indent(8) }}

    thinkube-page-template:
      mountPath: /usr/local/share/jupyterhub/custom-templates/page.html
      stringData: |
        {% raw %}{% extends "templates/page.html" %}

        {% block favicon %}
        <link rel="icon" type="image/svg+xml" href="{{ static_url('images/tk_ai.svg') }}">
        {% endblock %}

        {% block logo %}
        <div class="tk-brand">
          <img src="{{ static_url('images/tk_ai.svg') }}" alt="Thinkube AI Lab" id="jupyterhub-logo" />
          <span class="tk-brand-text">Thinkube AI Lab</span>
        </div>
        {% endblock %}

        {% block stylesheet %}
        {{ super() }}
        <link rel="stylesheet" href="{{ static_url('css/thinkube.css') }}" type="text/css"/>
        {% endblock %}{% endraw %}

    thinkube-css:
      mountPath: /usr/local/share/jupyterhub/static/css/thinkube.css
      stringData: |
        /* Thinkube AI Lab branding - Thinkube colors */
        :root {
          --tk-teal: #006680;
          --tk-teal-light: #008bad;
          --tk-teal-dark: #004d5c;
        }

        /* Navbar styling - warm off-white background, logo only */
        .navbar-default {
          background-color: #FEFDFB !important; /* Warm off-white from thinkube-style */
          border-color: #e8e6e3 !important;
          box-shadow: 0 1px 2px rgba(0, 0, 0, 0.05);
        }

        #thenavbar {
          background-color: #FEFDFB !important;
        }

        /* Hide navbar text links - keep only logo */
        .navbar-default .navbar-nav {
          display: none !important;
        }

        .navbar-default .navbar-brand {
          color: var(--tk-teal) !important;
          font-weight: 600;
        }

        /* Brand container - logo + text */
        .tk-brand {
          display: flex;
          align-items: center;
          gap: 12px;
        }

        .tk-brand-text {
          font-size: 18px;
          font-weight: 700;
          color: var(--tk-teal);
          letter-spacing: -0.02em;
        }

        /* Logo styling */
        #jupyterhub-logo {
          height: 40px;
          width: auto;
        }

        /* Primary buttons - teal theme */
        .btn-primary, .btn-jupyter {
          background-color: var(--tk-teal) !important;
          border-color: var(--tk-teal) !important;
        }

        .btn-primary:hover, .btn-jupyter:hover {
          background-color: var(--tk-teal-light) !important;
          border-color: var(--tk-teal-light) !important;
        }

        .btn-primary:active, .btn-jupyter:active {
          background-color: var(--tk-teal-dark) !important;
          border-color: var(--tk-teal-dark) !important;
        }

        /* Login page styling */
        #login-main .service-login {
          border-color: var(--tk-teal);
        }

        /* Announcement banner - teal background with white text */
        #announcement-banner {
          background-color: var(--tk-teal) !important;
          color: white !important;
          padding: 12px 20px;
          font-weight: 500;
          border-bottom: 2px solid var(--tk-teal-dark);
        }

proxy:
  secretToken: "{{ jupyterhub_proxy_token | default(lookup('password', '/dev/null chars=hex_digits length=64')) }}"
  service:
    type: ClusterIP
  https:
    enabled: false
  chp:
    image:
      name: quay.io/jupyterhub/configurable-http-proxy
      tag: "5.0.0"  # Use 5.0+ for websocket options support
    pdb:
      enabled: false

singleuser:
  defaultUrl: "/lab"

  # User ID configuration
  uid: 1000
  fsGid: 100  # Group ID for filesystem access

  # Init containers to fix permissions on JuiceFS volumes
  initContainers:
    - name: fix-permissions
      image: {{ harbor_registry }}/library/busybox:latest
      imagePullPolicy: IfNotPresent
      command:
        - sh
        - -c
        - |
          # First time setup - check marker in persistent area
          if [ ! -f "/home/thinkube/thinkube/.thinkube-initialized" ]; then
            echo "First time setup - creating examples directory..."

            # Create examples directory for editable copies of templates
            mkdir -p /home/thinkube/thinkube/notebooks/examples

            # Mark as initialized
            touch /home/thinkube/thinkube/.thinkube-initialized
            echo "First time setup complete"
          else
            echo "Already initialized"
          fi

          # Always fix permissions for persistent areas
          echo "Fixing permissions..."
          chown -R 1000:100 /home/thinkube/thinkube
          chmod -R 755 /home/thinkube/thinkube
          chown -R 1000:100 /home/thinkube/scratch 2>/dev/null || true
          chmod -R 755 /home/thinkube/scratch 2>/dev/null || true
          echo "Permissions fixed"
      securityContext:
        runAsUser: 0  # Run as root to fix permissions
      volumeMounts:
        - name: notebooks
          mountPath: /home/thinkube/thinkube/notebooks
        - name: datasets
          mountPath: /home/thinkube/thinkube/datasets
        - name: models
          mountPath: /home/thinkube/thinkube/models
        - name: scratch
          mountPath: /home/thinkube/scratch

    # Clone templates repository on each pod start (directly to templates mount - NO symlinks!)
    - name: clone-templates
      image: {{ harbor_registry }}/library/alpine-git:latest
      imagePullPolicy: IfNotPresent
      command:
        - sh
        - -c
        - |
          set -e
          echo "Cloning templates repository..."

          # Clone directly into the templates mount (emptyDir - fresh each pod start)
          git clone https://github.com/thinkube/thinkube-ai-examples.git /home/thinkube/thinkube/templates/thinkube-ai-examples

          # Flatten structure: /templates/thinkube-ai-examples/* → /templates/*
          mv /home/thinkube/thinkube/templates/thinkube-ai-examples/* /home/thinkube/thinkube/templates/
          rm -rf /home/thinkube/thinkube/templates/thinkube-ai-examples

          # Fix ownership
          chown -R 1000:100 /home/thinkube/thinkube/templates
          chmod -R 755 /home/thinkube/thinkube/templates

          # First time: copy templates to examples (editable copy)
          if [ ! -f "/home/thinkube/thinkube/notebooks/examples/.copied" ]; then
            echo "First time setup - copying templates to examples..."
            cp -r /home/thinkube/thinkube/templates/* /home/thinkube/thinkube/notebooks/examples/
            touch /home/thinkube/thinkube/notebooks/examples/.copied
            chown -R 1000:100 /home/thinkube/thinkube/notebooks/examples
            chmod -R 755 /home/thinkube/thinkube/notebooks/examples
            echo "Templates copied to examples/"
          fi

          echo "Templates ready at /home/thinkube/thinkube/templates/"
      securityContext:
        runAsUser: 0  # Run as root to clone and set permissions
      volumeMounts:
        - name: templates
          mountPath: /home/thinkube/thinkube/templates
        - name: notebooks
          mountPath: /home/thinkube/thinkube/notebooks

    # Query service discovery and generate environment file
    - name: service-discovery
      image: {{ harbor_registry }}/library/tk-service-discovery:latest
      imagePullPolicy: IfNotPresent
      command:
        - sh
        - /scripts/service-discovery-init.sh
      volumeMounts:
        - name: service-discovery-script
          mountPath: /scripts
        - name: service-config
          mountPath: /service-config
      securityContext:
        runAsUser: 0  # Need root to create config directory and fix permissions

    # Setup venvs: download from GitHub if needed (local disk - fast!), then register Jupyter kernels
    - name: setup-venvs
      image: {{ harbor_registry }}/library/tk-jupyter-base:latest
      imagePullPolicy: IfNotPresent
      command:
        - bash
        - -c
        - |
          set -e
          echo "Setting up virtualenv kernels (local storage - fast I/O)..."

          # Configuration
          VENVS_VERSION="{{ jupyter_venvs_version | default('v0.1.0') }}"
          GITHUB_REPO="thinkube/thinkube-venvs"
          GITHUB_RELEASE_URL="https://github.com/$GITHUB_REPO/releases/download/$VENVS_VERSION"

          # Detect architecture
          ARCH=$(uname -m)
          if [ "$ARCH" = "aarch64" ]; then
            ARCH_DIR="arm64"
          else
            ARCH_DIR="amd64"
          fi
          echo "Detected architecture: $ARCH_DIR"
          echo "Venvs version: $VENVS_VERSION"

          VENVS_BASE="/home/thinkube/venvs"
          VENVS_ARCH_DIR="$VENVS_BASE/$ARCH_DIR"

          # Check if venvs already exist and are the correct version
          VERSION_FILE="$VENVS_ARCH_DIR/.version"
          if [ -f "$VERSION_FILE" ] && [ "$(cat $VERSION_FILE)" = "$VENVS_VERSION" ]; then
            echo "Venvs $VENVS_VERSION already installed for $ARCH_DIR"
          else
            echo "Downloading venvs $VENVS_VERSION for $ARCH_DIR..."
            mkdir -p "$VENVS_ARCH_DIR"

            # Download and extract each system venv directly to local storage (fast!)
            for venv in fine-tuning agent-dev; do
              TARBALL_URL="$GITHUB_RELEASE_URL/$venv.tar.gz"
              echo "  Downloading $venv..."

              # Remove any existing incomplete venv first
              rm -rf "$VENVS_ARCH_DIR/$venv" 2>/dev/null || true

              # Extract directly to local storage (hostPath = fast local disk)
              if curl -fsSL "$TARBALL_URL" | tar -xzf - -C "$VENVS_ARCH_DIR/"; then
                echo "    ✓ $venv downloaded and extracted"
              else
                echo "    ✗ Failed to download $venv from $TARBALL_URL"
                echo "    Venvs may not be available yet. Run 99_build_venvs.yaml and upload to GitHub."
                # Continue without failing - user can still use base Python
              fi
            done

            # Record installed version
            echo "$VENVS_VERSION" > "$VERSION_FILE"
            echo "Venvs $VENVS_VERSION installed for $ARCH_DIR"
          fi

          # Create symlink to current architecture
          ln -sfn "$VENVS_ARCH_DIR" "$VENVS_BASE/current"
          echo "Created symlink: $VENVS_BASE/current -> $ARCH_DIR"

          # Register each venv as a Jupyter kernel (system venvs + custom venvs)
          # Use --prefix to install kernels to shared venvs volume (accessible by main container)
          KERNELS_DIR="$VENVS_BASE/kernels"
          mkdir -p "$KERNELS_DIR"

          # Register system venvs (downloaded from GitHub)
          for venv in "$VENVS_ARCH_DIR"/*/; do
            if [ -d "$venv" ] && [ -f "$venv/bin/python" ]; then
              VENV_NAME=$(basename "$venv")
              echo "Registering kernel: $VENV_NAME"
              "$venv/bin/python" -m ipykernel install \
                --prefix="$VENVS_BASE" \
                --name="$VENV_NAME" \
                --display-name="$VENV_NAME ($ARCH_DIR)" \
                2>/dev/null || echo "  Note: kernel may already be registered"
            fi
          done

          # Register custom venvs (built via thinkube-control, synced to all GPU nodes)
          # Only register custom venvs that match the current architecture (via .arch file)
          CUSTOM_VENVS_DIR="$VENVS_BASE/custom"
          if [ -d "$CUSTOM_VENVS_DIR" ]; then
            for venv in "$CUSTOM_VENVS_DIR"/*/; do
              if [ -d "$venv" ] && [ -f "$venv/bin/python" ]; then
                VENV_NAME=$(basename "$venv")

                # Check architecture marker for multi-arch support
                VENV_ARCH=""
                if [ -f "$venv/.arch" ]; then
                  VENV_ARCH=$(cat "$venv/.arch" 2>/dev/null || echo "")
                fi

                # Skip if venv was built for different architecture
                if [ -n "$VENV_ARCH" ] && [ "$VENV_ARCH" != "$ARCH_DIR" ]; then
                  echo "Skipping custom kernel $VENV_NAME - built for $VENV_ARCH, current arch is $ARCH_DIR"
                  continue
                fi

                # Register kernel with architecture info
                DISPLAY_NAME="$VENV_NAME (custom"
                if [ -n "$VENV_ARCH" ]; then
                  DISPLAY_NAME="$DISPLAY_NAME, $VENV_ARCH"
                fi
                DISPLAY_NAME="$DISPLAY_NAME)"

                echo "Registering custom kernel: $VENV_NAME as '$DISPLAY_NAME'"
                "$venv/bin/python" -m ipykernel install \
                  --prefix="$VENVS_BASE" \
                  --name="$VENV_NAME" \
                  --display-name="$DISPLAY_NAME" \
                  2>/dev/null || echo "  Note: kernel may already be registered"
              fi
            done
          fi

          # Fix ownership for venvs (includes kernels now)
          chown -R 1000:100 "$VENVS_BASE" 2>/dev/null || true

          # Jupyter looks for kernels at $JUPYTER_PATH/kernels, but ipykernel install --prefix
          # creates them at $JUPYTER_PATH/share/jupyter/kernels. Create symlink to fix this.
          if [ -d "$VENVS_BASE/share/jupyter/kernels" ] && [ ! -L "$VENVS_BASE/kernels" ]; then
            rm -rf "$VENVS_BASE/kernels" 2>/dev/null || true
            ln -sf share/jupyter/kernels "$VENVS_BASE/kernels"
            echo "Created symlink: $VENVS_BASE/kernels -> share/jupyter/kernels"
          fi

          echo "Kernels registered to: $VENVS_BASE/share/jupyter/kernels"
          ls -la "$VENVS_BASE/share/jupyter/kernels/" 2>/dev/null || echo "  (no kernels yet)"

          # Add Thinkube AI icon to all kernels (both 32x32 and 64x64 PNG required by JupyterLab)
          # Base64-encoded tk_ai PNG icons
          TK_ICON_64_B64="iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAMAAACdt4HsAAAAIGNIUk0AAHomAACAhAAA+gAAAIDoAAB1MAAA6mAAADqYAAAXcJy6UTwAAAJGUExURQAAAABoggBmgABmgABmgABlgABkgABohABmgABmgABmgABmmQBlgQBmgABmgACAgABogABmgABngABkgABmhQBmgABogABngABmgQBkggBmgABmgABqgABngABmgABnggBmgABmiABmgABmgABmgABtgABngQBmgABmgAAAAABkgwBmgABmgABpgwCAgABmgABmgABmgABngABmgABlgQBngQBngABmgABmgABmgABmgQBmgABlgABmgABngABlggBmgABmgABkhQBmgABmgABngABmggBmgQBmgABngQBngABjgABmgABmgABmgQBmgABmgQBqgABngABmgABtkgBlgQBjhABmgABmgABmgABmgQBkgABngABngABmgABmgwBlgABmgABngABmgQBmgABmgQBlgABlgABmgABmgABlgABngABkgABmgABmgABmgABmgQBlgQBmgABmgABrhgBmgABngQBlgABmgABmgABmgABngQBVqgBmgABxjgBmgABlgABmgABngQBmgQBmgABdiwBmgABigABhhgBmgQBngABnggBlgABlgQBlggBlgABmgABlgABlgABlgABmgABlgABlggBmgABmgABmgABmgABkggBlgABlgABjgABmgABmgABmgABlgABmgABogABlgABlgABlgABmgABmgABmgABmgABmgABmgABmgABlgABmgABngABngQBggABngABlgQBggABmgABogABngwBmgABngABphwBoggBngABmgQBmgAAAAPsJvWMAAADAdFJOUwA7rOnwsC4bov0UBW/y7QJA1Mw4Gf4Wcmk90s4YXuw5ww+R/PsOTePmASG2vCcEdvX6s3hdYZX3+Z5rbo3ffCvLpRfbj1Q3dfhtgRLFjFWJfQzC3Ad5H/G312ccPq7eI37ikHGnf4ir9sAmqULgmchLZaP0E45jTFrv00MDmwnrdK13QdgLsRoVcyovOkc1MNCXumCAvz+56IdkM0RqJITzloPGLMROYt3u5ZiFCueh6rhFEJ9RCM02JdlKETFme6S97e4AAAABYktHRACIBR1IAAAAB3RJTUUH6QwQDiQGemMz7QAABCNJREFUWMOdV/df00AUP20FKm1BtDhRoRVBEAsqxT0QwY2COCpqHYgDK9ZVRetCxT0qOBBRUcG9wK3vT/Pukl4ul9C0fn9oku973+8n9+69JEUoGgYMNJkHJaD/RWISEFiSBv+XPNlqAxl2a0r8+tQhwCFtaJzyYUkOUMGRlB7P3Q+3gQb2ETGvY+Qo0MXo2NYxRrx7fh3G+5GSMRaiwHA/xo0GA4yP1leZWU4jPYAra0J/+uyJxnKCnIxcPfmkvNjkBPmTNfKCKW51TmHR1GnTiz0AJTNmzpo9R3DwzE1W6+fNFzIWLKR8KcAicswtyxESRi3m9eUVQniJHMAGS6WzZcuFFMsKZUdXimtchRSDSvl0tegAI5jBGiFSVc0ZrI2c16wT0tYzgw1CZCPmvJtkg3J8qN28Bf9uFdI2MAOfOuDwYm4bXQc22I4bDGAKvtgiGPiYgbr9XTsItxRgJzXA9RwGUEe4XeoxMzGDep7evYdye3Bf7N3XQEtVA7CfkpP9fKaZGfCbuLdAJg/YJaKR3vtBiZzAl8vDDEoUMnCIsd7DtOxH6B3kye1/lO8EPQNla8jWHwtSg3QcOH6CUk1BA4OT5PoUi5RSAxQiodOUSVNynXoGZ8hcBjYKBglna+vBcY4w5/UMChUyhKdsMYSa1QZkA0JwAR8ucjvp0jMYm4jQJYAWjQHaCTnEIBTdwHSZXPvBdkVjUAY2crjqYcmFOgap9PqaA9x11wWDG3CTHpfoGLgi1E2ZaCErvSUZ3L7jpVxumI4XSmZDWaI1GBhhxp0JOlskA4y7rXx3+LUGbAltSECpPJ9rOY69+Cq0fXAvWWtw/wF+XFseMiqdvTuUWVAK26g1wEUMs2HCWMSS3YxTxrm+Xc+g2gTrmmTmkdJJAZbm5gbkjqLueJznlrZxEECHxJVz09jJMgMcWxkhM59ItSUGTwEuSmwll2pjBp0cO03mCp7h4q8xSwZdz4tlupFLVZ5I/AfNC5l7CeDvQt1KJyKhDwGCjDVzrFXm2mAIHskG0aCYS+1hLPeUAb/MVdEt9QkGp15xqXZG93AszJK4u07yXfdKMHjNZ4YYbedpy5u3hJtURn7fqQwOvecT4QMLCN9VPbu6IpEQZzCy1qPOU14smk+bLK46zOCjmHWPGRwRQ555SodEDD5pPh+3KourE4NzpEV8xnN6UprQ1qCQ4jzNV/dalRB2fTnW/uI1FXXP7e215luEhKpUVX+gvrBQoeiw7F+NRHTE8Zn39RvSQ3ZDbHKztQnp4224IgZ9kRf1j+9tRvIfn1B0ZPuiyd3hamSE5nBJv/qiGkM5QcJzfXn3z5jkGLkZJq28MKs5Vj3GryfiX4/1HXHICX4/4+W+jDjlGCl/2HMm8Lcvfj1Zh/wXKsba62Hox05bfmvUlH8ff2epAz+4PgAAAABJRU5ErkJggg=="
          TK_ICON_32_B64="iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAMAAABEpIrGAAAAIGNIUk0AAHomAACAhAAA+gAAAIDoAAB1MAAA6mAAADqYAAAXcJy6UTwAAAG/UExURQAAAABggABngQBmgABmgABlgQCAgABkgABmgABmgABmgABmgABngABmgABlgABigABmmQBmgABmgABmgABVgABngQBmgABkggBphwBmgABmgABkhQBmgABmgABmgABlgABmgABmgABmgABmgABmgABmgQAAAABkgABlgABlgABtkgBmgABmgABhhgBmgABngABpgABngABngABVqgBmgABkgwBlgQBmgABlgABmgABkgwBmgABmgABmgQBmgABngABmgABmgABmgABmgABmgABmgABmgABkgABrhgBmgABmgABmgABngABngABmgACAgABlgABmgABqhABmgABogABmgABmgABngABmgABmgABmgABlgQBlggBmgABlgABmgABmgABngABngwBngABqgABmgABjgABmgABmgABmgQBngABmgABlgABmgABoggBmgABxjgBlgABmgABogABlgABmgABpgwBlggBnggBmgABlgABmgABjhABmgABnggBmgABmgABiiQBmgABngABlgABmgABqgABmgABjgABmgABngABlgABmgABmgABngABmgABmgABlgABmgABmgAAAADGQD8QAAACTdFJOUwAId9/pbwRC1thGHq7+phoF8vSABkPSPRGZohda6rzE/cG+9+9nARy1gwftxRWvwiKavQPoIXm5apgp6/Fzpz7utI7IevPwOBOl/LGLVNwCsHgd7BbLCkr79sNlNfXJ+BRoJccMoyT62324+bqtMc0JTOA2MOQnPy+sjeEfqDnR5g3ZZli3GGwSkbM6z9Ve1Nd05WggvGcAAAABYktHRACIBR1IAAAAB3RJTUUH6QwQDiQGemMz7QAAAfRJREFUOMt1U+tfEkEUPYmmQpssopWZgRIpa0rRUyk1K18ZK2riA1HLqOhlD620tEx7WFnOP9zMnYF1Cc6H/Z17z5mZO3vvAPtxoMRRWnYQRVFewTgqnUVk1yEmoR2uKiC7y3SWg6fam6/XVDIbao/Y5KPHNJYHre64pdefkEm94aSP+RubZBQ4FZTy6Wa1qCUEGK3AmTaVaA+ToVGFZ0VgnOOfSHaJnwznVXQBF8MwLuHyFXSoVCcZOolHrwLXHF3dPbjeC9y4SclbZOgm3sdZ/8AgG8LtCk6HZdVkiAp6B4hxXqWbGGGtcURoCw8ZPIKOBsfGI6JIE3cn2hvgTYjsJBno2lOhSGBaGoCZJLyzIpsig/Cm5oD5RNsCjHtx4P4i8ED82zQZfJw9FGTmURwG0x7LXgZ4ejC3QybbYOOJ0/FUkGcst0Na0OfUmhdLHhPzaT5TrpcimyFDhq78irPXb6IJE8vjbmCFWUWmiL/lzP+uP2DKo5yUbCI+Sfw9Z6tr0JXhA7P+Q1I2ZqAPvNIkGT4OqdEjw7pq3cYnUZAwzGXH8zMZwr0qnOpa2NQ2vnQsZZdsbcvzYpsGKwD9azA3lN+++/7Tf7hsc73z0y7/Ks9/GLHfo5acrA4VeFvbdVrh3S2s7Ar5Tw2Kwv13b6/E/iz/AYuV/qUBBBs9AAAAAElFTkSuQmCC"
          for kernel_dir in "$VENVS_BASE/share/jupyter/kernels"/*/; do
            if [ -d "$kernel_dir" ]; then
              # Replace kernel icons with Thinkube AI logo (both sizes required by JupyterLab)
              echo "$TK_ICON_64_B64" | base64 -d > "$kernel_dir/logo-64x64.png"
              echo "$TK_ICON_32_B64" | base64 -d > "$kernel_dir/logo-32x32.png"
              # Remove old SVG to avoid conflicts
              rm -f "$kernel_dir/logo-svg.svg"
              echo "Added Thinkube icons to $(basename "$kernel_dir")"
            fi
          done

          echo "Venv setup complete!"
      volumeMounts:
        - name: venvs
          mountPath: /home/thinkube/venvs
      securityContext:
        runAsUser: 0  # Need root to create symlinks and fix permissions

  # ServiceAccount for querying ConfigMaps
  serviceAccountName: jupyter-service-discovery

  # MANDATORY: JuiceFS storage - no fallbacks
  storage:
    type: none  # We define volumes manually
    # Hybrid storage approach: JuiceFS (persistent) + local scratch (performance)
    extraVolumes:
    # JuiceFS for persistent notebooks (MANDATORY)
    - name: notebooks
      persistentVolumeClaim:
        claimName: jupyterhub-notebooks-pvc

    # JuiceFS for datasets (MANDATORY)
    - name: datasets
      persistentVolumeClaim:
        claimName: jupyterhub-datasets-pvc

    # JuiceFS for user models (MANDATORY)
    - name: models
      persistentVolumeClaim:
        claimName: jupyterhub-models-pvc

    # MLflow artifacts - SHARED with MLflow (same JuiceFS volume, zero duplication!)
    # Users can access MLflow models via POSIX at /home/thinkube/thinkube/mlflow/
    - name: mlflow-artifacts
      persistentVolumeClaim:
        claimName: jupyterhub-mlflow-artifacts-pvc

    # Local node storage for venvs (fast I/O - no JuiceFS small file bottleneck)
    # Each GPU node has its own copy, synced via rsync after custom venv builds
    - name: venvs
      hostPath:
        path: /var/lib/jupyterhub-venvs
        type: DirectoryOrCreate

    # Thinkube icons (from ConfigMap)
    - name: thinkube-icons
      configMap:
        name: thinkube-icons
        defaultMode: 0644

    # Local scratch for fast I/O (per-pod temporary)
    - name: scratch
      emptyDir:
        sizeLimit: 100Gi

    # Templates repository (cloned fresh on each pod start - NO symlinks!)
    - name: templates
      emptyDir:
        sizeLimit: 1Gi

    # Service discovery script (from ConfigMap)
    - name: service-discovery-script
      configMap:
        name: jupyter-service-discovery-script
        defaultMode: 0755

    # Service config output (shared between init container and main container)
    - name: service-config
      emptyDir: {}

    extraVolumeMounts:
    # Primary notebook storage (JuiceFS - persistent across nodes)
    - name: notebooks
      mountPath: /home/thinkube/thinkube/notebooks

    # Datasets directory (JuiceFS - persistent across nodes)
    - name: datasets
      mountPath: /home/thinkube/thinkube/datasets

    # User models directory (JuiceFS - persistent across nodes)
    - name: models
      mountPath: /home/thinkube/thinkube/models

    # MLflow artifacts (JuiceFS - SHARED with MLflow, zero duplication!)
    # Access MLflow models directly: /home/thinkube/thinkube/mlflow/artifacts/
    - name: mlflow-artifacts
      mountPath: /home/thinkube/thinkube/mlflow

    # Thinkube icons for notebooks (from ConfigMap)
    - name: thinkube-icons
      mountPath: /home/thinkube/thinkube/icons

    # Fast local scratch space (emptyDir - temporary)
    - name: scratch
      mountPath: /home/thinkube/scratch

    # Templates repository (emptyDir - cloned on pod start, NO symlinks!)
    # Read-only examples - users should copy to notebooks/ to edit
    - name: templates
      mountPath: /home/thinkube/thinkube/templates

    # Service discovery environment file (generated by init container)
    - name: service-config
      mountPath: /home/thinkube/.config/thinkube

    # Local node venvs (fast I/O - hostPath on each GPU node)
    # Init container downloads system venvs from GitHub releases
    - name: venvs
      mountPath: /home/thinkube/venvs

  # Image configuration
  image:
    pullPolicy: Always

  # Remove all node restrictions to allow scheduling on any GPU node
  nodeSelector: {}
  extraNodeAffinity: {}

  # Enable GPU support and AI capabilities
  extraEnv:
    # NVIDIA GPU support
    # NVIDIA_VISIBLE_DEVICES automatically set by NVIDIA device plugin based on GPU resource request
    NVIDIA_DRIVER_CAPABILITIES: "compute,utility"

    # JupyterLab configuration
    JUPYTER_ENABLE_LAB: "yes"
    GRANT_SUDO: "yes"

    # Include venvs directory in Jupyter's search path for kernels
    # Kernels are installed to /home/thinkube/venvs/share/jupyter/kernels by init container
    JUPYTER_PATH: "/home/thinkube/venvs"

    # Enable collaborative mode for YDoc/RTC integration (JupyterLab >= 4.1)
    JUPYTER_LAB_ENABLE_COLLABORATION: "true"

    # AI API keys - managed by thinkube-control secrets
    # Users should create ANTHROPIC_API_KEY secret in thinkube-control
    ANTHROPIC_API_KEY:
      valueFrom:
        secretKeyRef:
          name: anthropic-api-key
          key: value
          optional: true  # Don't fail pod startup if secret doesn't exist

  # Default resource limits (overridden by dynamic profiles)
  cpu:
    limit: 4
    guarantee: 1
  memory:
    limit: 8G
    guarantee: 2G

  # Allow privilege escalation for sudo
  allowPrivilegeEscalation: true

  # Network policy
  networkPolicy:
    enabled: false

  # Start timeout (20 minutes for image pull + venv download/extraction)
  startTimeout: 1200

  # Lifecycle hooks - add environment sourcing to shell configs
  lifecycleHooks:
    postStart:
      exec:
        command:
          - "bash"
          - "-c"
          - |
            # Add service-env-jh.sh to bashrc for interactive shells
            # (kernels get it via cmd override, but terminals need it too)
            if [ -f /home/thinkube/.config/thinkube/service-env-jh.sh ]; then
              if ! grep -q "service-env-jh.sh" /home/thinkube/.bashrc 2>/dev/null; then
                echo "" >> /home/thinkube/.bashrc
                echo "# Thinkube service discovery (auto-generated)" >> /home/thinkube/.bashrc
                echo "if [ -f ~/.config/thinkube/service-env-jh.sh ]; then" >> /home/thinkube/.bashrc
                echo "  source ~/.config/thinkube/service-env-jh.sh" >> /home/thinkube/.bashrc
                echo "fi" >> /home/thinkube/.bashrc
              fi
            fi

            # Add .secrets.env to bashrc for thinkube-control API keys
            if [ -f /home/thinkube/thinkube/notebooks/.secrets.env ]; then
              if ! grep -q "\.secrets\.env" /home/thinkube/.bashrc 2>/dev/null; then
                echo "" >> /home/thinkube/.bashrc
                echo "# Thinkube secrets (exported from thinkube-control)" >> /home/thinkube/.bashrc
                echo "if [ -f /home/thinkube/thinkube/notebooks/.secrets.env ]; then" >> /home/thinkube/.bashrc
                echo "  source /home/thinkube/thinkube/notebooks/.secrets.env" >> /home/thinkube/.bashrc
                echo "fi" >> /home/thinkube/.bashrc
              fi
            fi

# Ingress is handled separately in the deployment playbook
ingress:
  enabled: false

# Scheduling configuration (disabled for simplicity)
scheduling:
  userScheduler:
    enabled: false
  podPriority:
    enabled: false
  userPlaceholder:
    enabled: false

# Pre-puller configuration (disabled - images pulled on demand)
prePuller:
  hook:
    enabled: false
  continuous:
    enabled: false

# Culling configuration
cull:
  enabled: true
  timeout: 3600  # 1 hour idle timeout
  every: 600  # Check every 10 minutes
  maxAge: 0  # Don't cull based on age

# Debug mode (disable in production)
debug:
  enabled: false

# Global configuration
global:
  safeToShowValues: false